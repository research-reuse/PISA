{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Is there a relationship between countries’ wealth or spending on schooling and its students' performance in PISA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "Public schools were already known in antiquity and many societies developed them for years to finally end up with mandatory public institutions in the XIX century. Communities all over the world took care of educating youger generations providing a better future, because education has many benefits not only to individuals involved, but to society at large as well. Competition around countries is also not a new concept and education is only one of many factors taken into consideration while making a cross-national comparisons. Even though, it seems that nowadays people compete for jobs not just locally but internationally more than ever before and education is one of the most important instruments to gain an advantage. There is probably a number of factors that should be taken into account wondering what helps  students be prepared for further study, careers and life. One of the question we may ask is can we tell that there is a relationship between nation's wealth or its spending on education and students' performance? Let's try to answer this question using some open data sets, Python language and regression analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PISA?\n",
    "First thing you need to ask yourself is how to measure performance, meaning how to tell that one country is doing better than the other in general? The Organisation for Economic Co-operation and Development (<a herf=\"https://en.wikipedia.org/wiki/OECD\">OECD</a>) is putting a lot of resources to design a proper measure for this comparison. From year 2000 they are performing a complex study every three years.\n",
    "The Programme for International Student Assessment (<a href=\"https://en.wikipedia.org/wiki/Programme_for_International_Student_Assessment\">PISA</a>) evaluates the extent to which 15-year-old students, near the end of their compulsory education, have acquired key knowledge and skills that are essential for full participation in modern societies. The assessment focuses on the core school subjects of science, reading and mathematics. The assessment does not just ascertain whether students can reproduce knowledge; it also examines how well students can extrapolate from what they have learned and can apply that knowledge in unfamiliar settings, both in and outside of school. This approach reflects the fact that modern economies reward individuals not for what they know, but for what they can do with what they know. The findings from PISA allow policy makers around the world to gauge the knowledge and skills of students in their own countries in comparison with those in other countries, set policy targets against measurable goals achieved by other education systems, and learn from policies and practices applied elsewhere. \n",
    "\n",
    "The most interesting findings from the latest 2015 report are that:\n",
    "- Singapore outperforms all other participating countries/economies in science. Japan, Estonia, Finland and Canada are the four highest-performing OECD countries.\n",
    "- Nearly 20% of students in OECD countries, on average, do not attain the baseline level of proficiency in reading. This proportion has remained stable since 2009 [<a href=\"#source_1\">1</a>, p.3-4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting PISA dataset\n",
    "Now that we have a better understanding of the performance measure, we can move to extracting PISA dataset. The analysis is based on pandas data frames. <a href=\"https://pandas.pydata.org/pandas-docs/stable/\"> Pandas </a> is a Python package providing fast and flexible methods for doing practical, real world data analysis. \n",
    "\n",
    "Let's first import necessary libraries for the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "import wbdata\n",
    "import datetime\n",
    "#import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read in <a href=\"https://github.com/mklajnerok/PISA/tree/master/pisa_basic/data_files\">csv files</a> containing test results from year 2000 to 2015. We see that there are 3 separate files with science, reading and mathematics results, which is the way they are presented in <a href=\"https://data.oecd.org/pisa/reading-performance-pisa.htm#indicator-chart\">OECD database</a>. To get the sense of all the files at once we are going to read them in as a dictionary with keys of exam names and values of data frames containing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multi_csv_data(files_list):\n",
    "    \"\"\"Read multi csv files from data_files directory and\n",
    "    create df_dict dictionary containing separate data frames\n",
    "    :param files_list: list of csv files\n",
    "    :returns df_data: dictionary with string keys and data frames values\"\"\"\n",
    "    df_dict = {}\n",
    "    for f in files_list:\n",
    "        d = pd.read_csv('../data_files/{0}'.format(f))\n",
    "        df_dict[f.replace('.csv', '')] = d\n",
    "    return df_dict\n",
    "\n",
    "#read in files with PISA results, separate for every subject (all years)\n",
    "pisa_data = read_multi_csv_data(['pisa_math_2003_2015.csv', 'pisa_read_2000_2015.csv', 'pisa_science_2006_2015.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dict_summary(df_dict):\n",
    "    \"\"\"Print name, head and tail for each data frame in a df_dict dictionary\n",
    "    :param df_dict: dictionary with string keys and data frames values\"\"\"\n",
    "    for k, v in df_dict.items():\n",
    "        print('\\n' + k + '\\n')\n",
    "        print(v.head())\n",
    "        print(v.tail())\n",
    "\n",
    "#show summary of PISA results\n",
    "show_dict_summary(pisa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that raw data frames consist of many irrelevant features, so let's perform necessary cleansing by dropping some columns and renaming the existing ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dict_columns(df_dict, del_col):\n",
    "    \"\"\"In each data frame in a df_dict dictionary delete del_col columns\n",
    "    :param df_dict: dictionary with string keys and data frames values\n",
    "    :param del_col: list\"\"\"\n",
    "    for k, v in df_dict.items():\n",
    "        for i in del_col:\n",
    "            v.drop(i, axis=1, inplace=True)\n",
    "            \n",
    "#drop unnecessary columns\n",
    "drop_dict_columns(pisa_data, ['INDICATOR', 'SUBJECT', 'MEASURE', 'FREQUENCY', 'Flag Codes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dict_columns(df_dict, new_col):\n",
    "    \"\"\"In each data frame in a df_dict dictionary rename columns\n",
    "    :param df_dict: dictionary with string keys and data frames values\n",
    "    :param new_col: dictionary\n",
    "    \"\"\"\n",
    "    for k, v in df_dict.items():\n",
    "        v.rename(columns=new_col, inplace=True)\n",
    "\n",
    "#rename columns\n",
    "rename_dict_columns(pisa_data, {'LOCATION': 'Code', 'Value': 'test_score', 'TIME': 'Time'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest PISA results were collected in year 2015, so we can use a filter to extract those rows. In the next step we will get rid of dictionary form, because we reduced significantly the amount of data. We can now merge all the data frames in one, so we have results for math, reading and science for each country in a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_by_year(df_dict, year):\n",
    "    \"\"\"Create a copy of df_dict and extract rows for a given year\n",
    "    :param df_dict: dictionary with string keys and data frames values\n",
    "    :param year: int\n",
    "    :returns df_dict_year: dictionary with string keys and data frames values\n",
    "    \"\"\"\n",
    "    df_dict_year = df_dict.copy()\n",
    "    for k, v in df_dict_year.items():\n",
    "        v = df_dict_year[k]\n",
    "        v = v[v['Time'] == year]\n",
    "        df_dict_year[k] = v\n",
    "    return df_dict_year\n",
    "\n",
    "#extract PISA results for 2015\n",
    "pisa_2015 = filter_dict_by_year(pisa_data, 2015)\n",
    "\n",
    "def merge_dict_by_year(df_dict_year):\n",
    "    \"\"\"Take df_dict_year and merge each data frame in one based on Code, \n",
    "    then drop columns with year number\n",
    "    :param df_dict_year: dictionary with string keys and data frames values\n",
    "    :returns df_data_joined: data frame\"\"\"\n",
    "    df_data_joined = pd.DataFrame()\n",
    "    for k, v in df_dict_year.items():\n",
    "        if not df_data_joined.empty:\n",
    "            df_data_joined = pd.merge(df_data_joined, v, on='Code')\n",
    "        else:\n",
    "            df_data_joined = df_data_joined.append(v)\n",
    "    df_data_joined.drop(df_data_joined.columns[[1, 3, 5]], axis=1, inplace=True)\n",
    "    return df_data_joined\n",
    "\n",
    "#join all tests' results from 2015\n",
    "all_pisa_2015 = merge_dict_by_year(pisa_2015)\n",
    "\n",
    "def rename_columns(df_data, new_col):\n",
    "    \"\"\"Take df_data data frame and rename new_col columns\n",
    "    :param df_data: data frame\n",
    "    :param new_col: dictionary\"\"\"\n",
    "    df_data.rename(columns=new_col, inplace=True)\n",
    "\n",
    "#rename column labels\n",
    "rename_columns(all_pisa_2015, {'test_score_x': 'math', 'test_score_y': 'read', 'test_score': 'science'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pisa_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at this data frame it's not always obvious, what is the name of each country, so for improving clarity let's include countries’ names in the table. To construct a map for converting from country alpha3 code to country name you can use <a href=\"https://pypi.python.org/pypi/pycountry\">pycountry library</a>. The dictionary required some adjustments to be compatible with other data sources used later in the analysis. It turns out countries’ official names vary a lot among different databases. By the way, did you hear about <a href=\"https://www.forbes.com/sites/francistapon/2017/05/22/czechia-has-won-the-czech-republic-name-debate/#a49f1d17d664\"> Czechia</a>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_code_dict():\n",
    "    \"\"\"Use pycountry library to create a map for converting from country name to country code\n",
    "    :returns name_code_dict: dictionary\"\"\"\n",
    "    name_code_dict = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "    dict_adjust = {'Czech Republic': 'CZE', 'Hong Kong SAR, China': 'HKG', 'Korea, Rep.': 'KOR',\n",
    "                      'Macao SAR, China': 'MAC', 'OECD members': 'OED', 'Slovak Republic': 'SVK',\n",
    "                  'China, Hong Kong Special Administrative Region': 'HKG', 'China, Macao Special Administrative Region': 'MAC',\n",
    "                  'Republic of Korea': 'KOR', 'United Kingdom of Great Britain and Northern Ireland': 'GBR',\n",
    "                  'United States of America': 'USA', 'OECD members': 'OAVG'}\n",
    "    name_code_dict.update(dict_adjust)\n",
    "    return name_code_dict\n",
    "\n",
    "def reverse_dict(dictionary):\n",
    "    \"\"\"Reverse other map for converting from country code to country name\n",
    "    :param dictionary: dictionary\n",
    "    :returns reversed_dict: dictionary\"\"\"\n",
    "    reversed_dict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return reversed_dict\n",
    "\n",
    "def add_country_name(df_data, code_name_dict):\n",
    "    \"\"\"Take df_data, add a column with country name and fill it using code_name_dict\n",
    "    :param df_data: data frame\n",
    "    :param code_name_dict: dictionary\"\"\"\n",
    "    mapper = lambda x: code_name_dict[x]\n",
    "    df_data.insert(loc=0, column='Country', value=df_data.loc[:, 'Code'].copy())\n",
    "    df_data['Country'] = df_data['Country'].apply(mapper)\n",
    "\n",
    "#add column with country name\n",
    "name_code_dict = create_name_code_dict()\n",
    "code_name_dict = reverse_dict(name_code_dict)\n",
    "add_country_name(all_pisa_2015, code_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pisa_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very nice to see all the spread in data among different test subjects, but for the basic analysis it seems right to calculate an average result for every country. Including relations between each exam parts could cause overplotting and extending unnecessarily the regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(df_data):\n",
    "    \"\"\"Takes a copy of df_data and calculate average pisa result for a given country,\n",
    "    with formula: (math+read+2*science)/4\n",
    "    :param df_data: data frame\n",
    "    :returns df_data_new: data frame\n",
    "    \"\"\"\n",
    "    df_data_new = df_data.copy()\n",
    "    df_data_new.insert(loc=2, column='ave_result', value=0)\n",
    "    df_data_new['ave_result'] = round((df_data['math'] + df_data['read'] + df_data['science']) / 3, 0)\n",
    "    df_data_new.drop(['math', 'read', 'science'], axis=1, inplace=True)\n",
    "    return df_data_new\n",
    "\n",
    "#get average pisa result for every country\n",
    "all_pisa_2015_ave = get_average(all_pisa_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results in ascending order\n",
    "all_pisa_2015_ave.sort_values(['ave_result'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting GDP dataset \n",
    "\n",
    "Now that we have PISA results, we can move to the next step, which is extracting GDP data. For many years we accepted <a href=\"https://en.wikipedia.org/wiki/Gross_domestic_product\">GDP</a> as an honest measure for determining the economic performance of a whole country, so we will use GDP per capita indicator to answer a question - do richer countries perform better in the assessment? \n",
    "\n",
    "First of all, let's generate a list of codes for countries we have test results for. We are going to use it as an argument in our next function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codes_list(df_data):\n",
    "    \"\"\"Create a list of countries codes from column Code in df_data\n",
    "    change code for OECD members from OAVG to OED\n",
    "    :param df_data: data frame\n",
    "    :returns codes_list: list\"\"\"\n",
    "    codes_list = df_data['Code'].tolist()\n",
    "    codes_list.remove('OAVG')\n",
    "    codes_list.append('OED')\n",
    "    return codes_list\n",
    "\n",
    "#get list of countries, who took PISA test\n",
    "countries_codes = get_codes_list(all_pisa_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, thanks to World Bank API and Python <a href=\"http://wbdata.readthedocs.io/en/latest/\">wbdata</a> wrapper, we can easilly load desired data directly to Python shell knowing which indicators we are interested in. To learn more about this helpful tool go to this <a href=\"https://blogs.worldbank.org/opendata/accessing-world-bank-data-apis-python-r-ruby-stata\"> blog post</a>. \n",
    "\n",
    "Working with a country Gross Domestic Product indicator you realize there are namy versions of it and you have to make a decision which one is suitable for your purpose. I decide to go for  GDP per capita <a href=\"https://en.wikipedia.org/wiki/Purchasing_power_parity\">purchasing power parity</a> based, because it enables better comparison between countries wealth. If you are not familiar with the difference between nominal and PPP GDP I suggest reading this <a href=\"https://applebutterdreams.wordpress.com/the-difference-between-gdp-nominal-and-gdp-ppp/\"> article</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_wbdata(countries, indicators, year_from, year_to):\n",
    "    \"\"\"Create data frame for given list of countries, indicators and dates using World Bank API\n",
    "    :param countries: list of codes\n",
    "    :param indicators: dict {ind_code : ind_name}\n",
    "    :param year_from: starting year\n",
    "    :param year_to: ending year\n",
    "    :returns df_data: multi index data frame\n",
    "    \"\"\"\n",
    "    data_date = (datetime.datetime(year_from, 1, 1), datetime.datetime(year_to, 1, 1))\n",
    "    df_data = wbdata.get_dataframe(indicators, country=countries, data_date=data_date, convert_date=False)\n",
    "    return df_data\n",
    "\n",
    "#get GDP PPP data (NY.GDP.PCAP.PP.KD - GDP per capita, PPP (constant 2011 international $))\n",
    "gdp_ppp = load_from_wbdata(countries_codes, {'NY.GDP.PCAP.PP.KD':'gdp_ppp'}, 2003, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our data frame needs some wrangling. Let's first filter all the rows for the same year as PISA results, which 2015. As you see the GDP data came as a multiIndex data frame. We can reset the index to simple integers and use column labels to perform various operations. At the end we will add a column with country code to enable merging later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_year(df_data, year):\n",
    "    \"\"\"Create a copy of df_data and extract rows for a given year\n",
    "    :param df_data: data frame\n",
    "    :param year: string\n",
    "    :returns df_data_year: data frame\"\"\"\n",
    "    df_data_year = df_data.xs(year, level='date').copy()\n",
    "    return df_data_year\n",
    "\n",
    "#get GDP PPP for 2015\n",
    "gdp_ppp_2015 = filter_by_year(gdp_ppp, '2015')\n",
    "\n",
    "#reset index \"country'\n",
    "gdp_ppp_2015.reset_index(level=['country'], inplace=True)\n",
    "\n",
    "#rename column label\n",
    "rename_columns(gdp_ppp_2015, {'country': 'Country'})\n",
    "\n",
    "def add_country_code(df_data, name_code_dict):\n",
    "    \"\"\"Take df_data, add a column with country code and fill it using name_code_dict\n",
    "    :param df_data: data frame\n",
    "    :param name_code_dict: dictionary\"\"\"\n",
    "    mapper = lambda x: name_code_dict[x]\n",
    "    df_data.insert(loc=1, column='Code', value=df_data.loc[:, 'Country'].copy())\n",
    "    df_data['Code'] = df_data['Code'].apply(mapper)\n",
    "\n",
    "#add column with country code\n",
    "add_country_code(gdp_ppp_2015, name_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results in descending order\n",
    "gdp_ppp_2015.sort_values(['gdp_ppp'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis for average PISA results and GDP per capita\n",
    "\n",
    "Regression analysis should be able to produce an equation that will predict dependent variable (average PISA result) using one or more independent variables (GDP per capita). Using just one independent variable seems insufficient to explain all the variability in dependent variable, but we can still try to understand how it could affect the outcome on its own. When running your regression, you are trying to discover whether the coefficients on your independent variables are really different from 0 (so the independent variables are having a genuine effect on your dependent variable) or if alternatively any apparent differences from 0 are just due to random chance. The null (default) hypothesis is always that each independent variable is having absolutely no effect (has a coefficient of 0) and you are looking for a reason to reject this theory [<a href=\"#source_6\">6</a>].\n",
    "We are going to use Ordinary Least Squares (<a href=\"https://en.wikipedia.org/wiki/Ordinary_least_squares\">OLS</a>) method to model a relationship between PISA results and GDP per capita. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing regression we need to take a quick look at our cleaned data in order to pre-examine the relationship. Let's merge both data frames using alpha3 code values and then look at the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df_onCode(df_data1, df_data2):\n",
    "    \"\"\"Merge two data frames on Code column, drop double country column\n",
    "    :param df_data1: data frame\n",
    "    :param df_data2: data frame\n",
    "    :returns df_joined: data frame\"\"\"\n",
    "    df_joined = pd.merge(df_data1, df_data2, on='Code')\n",
    "    df_joined.drop(['Country_y'], axis=1, inplace=True)\n",
    "    return df_joined\n",
    "\n",
    "#merge data\n",
    "pisa_ave_gdp_ppp = merge_df_onCode(all_pisa_2015_ave, gdp_ppp_2015)\n",
    "\n",
    "#rename column label\n",
    "rename_columns(pisa_ave_gdp_ppp, {'Country_x': 'Country'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_scatterplot(df_data, variables, color, title, xlabel, ylabel):\n",
    "    \"\"\"Take df_data and plot for chosen columns in variables list\n",
    "    :param df_data: data frame\n",
    "    :param variables: list of strings\n",
    "    :param color: string\"\"\"\n",
    "    plt.scatter(x=df_data[variables[0]], y=df_data[variables[1]], color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "matplotlib.style.use('ggplot')\n",
    "plot_ave_gdp_ppp = show_scatterplot(pisa_ave_gdp_ppp, ['gdp_ppp', 'ave_result'], 'y',\n",
    "                                    'PISA 2015 average result vs. GDP per capita', 'GDP per capita (PPP $)',\n",
    "                                    'average test result (points)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the points in the plot are not necessarily forming a straight line, which is a reminder that we should take log of GDP data to enable linear regression analysis. It helps to make our distribution less skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_log(df_data, columns):\n",
    "    \"\"\"Create a copy of df_data and take log of values from columns\n",
    "    :param df_data: data frame\n",
    "    :param columns: list of strings\"\"\"\n",
    "    log_data = df_data.copy()\n",
    "    for i in columns:\n",
    "        log_data[i] = np.log(log_data[i])\n",
    "        rename_columns(log_data, {i: str(i) + '_log'})\n",
    "    return log_data\n",
    "\n",
    "#take log from GDP values\n",
    "pisa_ave_gdp_ppp_log = take_log(pisa_ave_gdp_ppp, ['gdp_ppp'])\n",
    "\n",
    "#plot with GDP log\n",
    "plot_ave_gdp_ppp_log = show_scatterplot(pisa_ave_gdp_ppp_log, ['gdp_ppp_log', 'ave_result'], 'm', \n",
    "                                        'PISA 2015 average result vs. GDP per capita (log)', 'GDP per capita (log)',\n",
    "                                    'average test result (points)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data form something resembling straight line, but still there is one dot which seems to be an outlier from the linear model - it’s Luxembourg. It's a country that has been considered second richest in the world for many years lately. Luxembourg is a place with relative high income per capita and according to Business Insider report this is due to the large number of people working in the tiny nation while living in surrounding France, Germany and Belgium (it can be even one quarter of the entire population, who are not included in GDP calculation [<a href=\"#source_7\">7</a>]). Excluding Luxembourg from regression might seem like a good idea, that's why we will perform the analysis in two versions to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform OLS \n",
    "model_ave_gdp_ppp_log = smf.ols(formula='ave_result ~ gdp_ppp_log', data=pisa_ave_gdp_ppp_log).fit()\n",
    "model_ave_gdp_ppp_log.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave LUX out as an outlier\n",
    "pisa_ave_gdp_ppp_log_lux = pisa_ave_gdp_ppp_log[pisa_ave_gdp_ppp_log['Code'] != 'LUX']\n",
    "\n",
    "#perform OLS without Luxembourg\n",
    "model_ave_gdp_ppp_log_lux = smf.ols(formula='ave_result ~ gdp_ppp_log', data=pisa_ave_gdp_ppp_log_lux).fit()\n",
    "model_ave_gdp_ppp_log_lux.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first version the R-squared value indicate that there is a 0.62 correlation between GDP per capita and average PISA test results. When we  exclude Luxembourg from the dataset the percentage of the variables that is explained by a linear model increases to 0.70, which is still moderate.\n",
    "\n",
    "Interpreting our model coefficients we can say that after increasing GDP per capita by 1%, the average PISA result should go up by 0.68 points.\n",
    "Other important features of regression output are:\n",
    "- standard error, which is an estimate of the standard deviation of the coefficient, the amount it varies across cases,\n",
    "- p-value, which helps to determine the significance of the results. You get it by comparing the t statistic on your variable (the coefficient divided by its standard error) with values in the Student's t distribution\n",
    "- confidence interval, which tells us where the most of our variables are distributed\n",
    "\n",
    "Our standard error oscillate in the surroundings of +/- 7 points. We have a very small p-value, which indicates strong evidence against the null hypothesis. It means we can reject the null hypothesis that GDP per capita does not have any effect on PISA results. In other words, you can say with a 100% probability of being correct that the variable is having some effect, assuming your model is specified correctly. Examining confidence interval, we see that we can be 95% confident that the \"true\" GDP per capita coefficient is between 0.53 and 0.82 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the curve we fitted into our data using regression coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data_mat(df_data1, df_data2, degree, title, x_label, y_label):\n",
    "    \"\"\"Plot data from df_data1 and df_data2 and try to fit a curve with a given degree using matplotlib.pyplot\n",
    "    :param df_data1: data frame\n",
    "    :param df_data2: data frame\n",
    "    :param degree: integer\n",
    "    :param x_label: string\n",
    "    :param y_label: string\"\"\"\n",
    "    pylab.plot(df_data1, df_data2, 'bo', label='Data')\n",
    "    pylab.title(title)\n",
    "    pylab.xlabel(x_label)\n",
    "    pylab.ylabel(y_label)\n",
    "    model = pylab.polyfit(df_data1, df_data2, degree)\n",
    "    est_y_vals = pylab.polyval(model, df_data1)\n",
    "    pylab.plot(df_data1, est_y_vals, 'r', label='Curve fit')\n",
    "    pylab.legend(loc='best')\n",
    "    pylab.show()\n",
    "\n",
    "#plot with curve\n",
    "lin_ave_gdp_ppp_log_lux = fit_data_mat(pisa_ave_gdp_ppp_log_lux ['gdp_ppp_log'], pisa_ave_gdp_ppp_log_lux ['ave_result'], 1,\n",
    "                                       'Regression analysis curve fit (without Luxembourg)', 'GDP per capita (log)',\n",
    "                                   'average test result (points)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing the first part up, we can say with a great certainty that countries' GDP per capita has some effect on it's average PISA test results. Unfortunately, even the correlation rate above 70% couldn't confirm that the relationship is causal, meaning it's still hard to say, if the wealth of the country is the reason that its students are the top performers in the assessment or it's the other way round. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting government expenses on education per student \n",
    "\n",
    "The previous part of analysis showed that there is a reason to believe that students in wealthier countries reach higher scores in the assessment, meaning, they are better prepared for full participation in society. There's probably number of factors influencing the outcome, but let's take a deeper look at one more - government expenses on education. We can ask a question, does higher public expenses influence exam results positively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an assumption that before each student sat at PISA test at the age of 15 he or she probably had spent 12 years on learning in various forms and institutions. Starting with kindergarten we can add up expenses per student according to another assumption, that every pupil spent 3 years in pre-primary institutions, 6 years in primary and then 3 more in lower- secondary. This way, we are going to calculate total cost of each student education before being evaluated. Let's first calculate yearly expenses per student on every level of education using two indicators:\n",
    "- Goverment expenditure on education in PPP $ from <a href=\"http://data.uis.unesco.org/\">UNESCO database</a>\n",
    "- Population of the official age (both sexes) form World Bank API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data_by_list(file, codes_list):\n",
    "    \"\"\"Read in a file from pisaprojectdatafiles directory and select rows with countries codes from codes_list\n",
    "    :param file: string\n",
    "    :param codes_list: list\n",
    "    :returns: df_data: data frame\"\"\"\n",
    "    df = pd.read_csv('../data_files/{0}'.format(file))\n",
    "    df_data = pd.DataFrame()\n",
    "    for i in codes_list:\n",
    "        df_data = df_data.append(df.loc[df['LOCATION'] == i], ignore_index=True)\n",
    "    return df_data\n",
    "\n",
    "#read in file for given countries (source UNESCO database)\n",
    "gov_edu_expenses = read_csv_data_by_list('gov_exp_edu_ppp.csv', countries_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_edu_expenses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cleaning. First we need to filter only rows with expenses for 3 levels of education, we are interested in. Then we will drop a few irrelevant columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_indicator(df_data, indicators):\n",
    "    \"\"\"Take df_data and select rows for given indicators, then append it to df_data_ind\n",
    "    :param df_data: data frame\n",
    "    :param indicators: list\"\"\"\n",
    "    df_data_ind = pd.DataFrame()\n",
    "    for i in indicators:\n",
    "        df_data_ind = df_data_ind.append(df_data.loc[df_data['EDULIT_IND'] == i], ignore_index=True)\n",
    "    return df_data_ind\n",
    "\n",
    "#select indicators: pre-primary, primary and lower secondary\n",
    "basic_edu_exp = filter_by_indicator(gov_edu_expenses, ['X_PPP_02_FSGOV', 'X_PPP_1_FSGOV', 'X_PPP_2_FSGOV'])\n",
    "\n",
    "def drop_columns(df_data, del_col):\n",
    "    \"\"\"Take df_data and drop del_col columns from it\n",
    "    :param df_data: data frame\n",
    "    :param del_col: list\"\"\"\n",
    "    for i in del_col:\n",
    "        df_data.drop(i, axis=1, inplace=True)\n",
    "\n",
    "#drop unnecessary column\n",
    "drop_columns(basic_edu_exp, ['TIME', 'Flag Codes', 'Flags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_edu_exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get columns with labels for each level of education we are going to pivot the table. We are get a data frame with multiIndex as a side effect, but we can easily reset the index in the next step. Lastly we will rearrange columns order and add new column with country code to improve readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot df to get similar structure like student_count_data\n",
    "basic_edu_exp = basic_edu_exp.pivot_table('Value', ['Country', 'Time'], 'EDULIT_IND')\n",
    "\n",
    "#reset multiindex \"Country' and 'Time'\n",
    "basic_edu_exp.reset_index(level=['Country', 'Time'], inplace=True)\n",
    "\n",
    "#rename column labels\n",
    "rename_columns(basic_edu_exp,\n",
    "                {'X_PPP_02_FSGOV': 'pre_primary_exp', 'X_PPP_1_FSGOV': 'primary_exp', 'X_PPP_2_FSGOV': 'lower_sec_exp'})\n",
    "\n",
    "#add new column with country code\n",
    "add_country_code(basic_edu_exp, name_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_edu_exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have global expenses on education, let's look for the number of students on each level in a given year. We're going to use population of the official age indicator, because it tells how many children were suitable for every level of education in a given year. It's just an approximation of the real number of students who attended to schools, but due to lack of entries in other data sets it is still quite informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file with student count for given countries\n",
    "edu_indicators = {'SP.PRE.TOTL.IN':'pre_primary_pop', 'SP.PRM.TOTL.IN':'primary_pop', 'SP.SEC.LTOT.IN':'lower_sec_pop'}\n",
    "basic_student_pop = load_from_wbdata(countries_codes, edu_indicators, 2003, 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_student_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to perform some minor cleaning like reseting the multiIndex or unifying labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset multiindex\n",
    "basic_student_pop.reset_index(level=['country', 'date'], inplace=True)\n",
    "\n",
    "#rename column labels\n",
    "rename_columns(basic_student_pop, {'country': 'Country', 'date': 'Time'})\n",
    "\n",
    "#change column order\n",
    "basic_student_pop = basic_student_pop[['Country', 'Time', 'pre_primary_pop', 'primary_pop', 'lower_sec_pop']]\n",
    "\n",
    "#add new column with country code\n",
    "add_country_code(basic_student_pop, name_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_student_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finally get government expenses per student we need to convert 'Time' column data type to enable further merging. We are going to merge the frames with 'right' method, because we want to stay with data only for years 2003-2014. After that we do the division and clean our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 'Time' column type to int\n",
    "basic_edu_exp['Time'] = basic_edu_exp['Time'].apply(np.int16)\n",
    "basic_student_pop['Time'] = basic_student_pop['Time'].apply(np.int16)\n",
    "\n",
    "#merge expenses and population, right method to stay with 2003-2014 period\n",
    "edu_data_joined = pd.merge(basic_edu_exp, basic_student_pop, how='right', on=['Code', 'Time'])\n",
    "\n",
    "#sort by 'Code' and 'Time' and reset index\n",
    "edu_data_joined.sort_values(['Code', 'Time'], ascending=[True, True], inplace=True)\n",
    "edu_data_joined.reset_index(level=0, drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_data_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_col_by_col(df_data, dividends, divisors):\n",
    "    \"\"\"Takes df_data and divides columns accordingly by rule dividends/divisors,\n",
    "    returns df_data_div only with divided columns\n",
    "    :param df_data: data frame\n",
    "    :param dividends: list of column labels\n",
    "    :param divisors: list of column labels\n",
    "    :returns: df_data_div: data frame\n",
    "    \"\"\"\n",
    "    df_data_div = df_data.copy()\n",
    "    for n in range(len(dividends)):\n",
    "        df_data_div[n] = df_data_div[dividends[n]] / df_data_div[divisors[n]] * 1000000\n",
    "    df_data_div.round(0)\n",
    "    drop_columns(df_data_div, [dividends + divisors])\n",
    "    drop_columns(df_data_div, ['Country_y'])\n",
    "    rename_columns(df_data_div, {'Country_x': 'Country', 0: 'pre_primary_per_student',\n",
    "                                          1: 'primary_per_student', 2: 'lower_sec_per_student'})\n",
    "    return df_data_div\n",
    "\n",
    "#divide total expenses by number of students\n",
    "edu_data_per_student = divide_col_by_col(edu_data_joined, ['pre_primary_exp', 'primary_exp', 'lower_sec_exp'],\n",
    "                                         ['pre_primary_pop', 'primary_pop', 'lower_sec_pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_data_per_student.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see NaN values started to occur and using .isnull() method we can check that there's more of them in the whole data frame. There are a few ways to deal with those lacks, but in this case it seems reasonable to make a use of Python build in function called <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\"> interpolate</a>. In order not to fill the gaps, where we don't have enough information let's make an assumption. For every country in the dataset first check the number of NaN in a given period we are interested in, for example years 2003-2005 for pre-primary education. If any lack occurs, examine the whole column for a given level of education. If the ratio of NaN is below 50% we will interpolate for the whole column, in other cases we will drop the country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_total_cost(df_data):\n",
    "    \"\"\"Takes df_data and estimates total cost per student during 12 years of education\n",
    "    :param df_data: data frame\n",
    "    :returns total_cost: total_cost df with country Code and total cost\"\"\"\n",
    "    total_cost = pd.DataFrame(columns=['Code', 'Total'])\n",
    "    country_list = df_data.Code.unique()\n",
    "    for country in country_list:\n",
    "        country_df = df_data.loc[df_data['Code'] == country].copy()\n",
    "        cost_pre_primary = sum_col(country_df, 0, 3, 'pre_primary_per_student')\n",
    "        cost_primary = sum_col(country_df, 3, 9, 'primary_per_student')\n",
    "        cost_second = sum_col(country_df, 9, 12, 'lower_sec_per_student')\n",
    "        cost_temp = [cost_pre_primary, cost_primary, cost_second]\n",
    "        for i in cost_temp:\n",
    "            if i > 0:\n",
    "                sum_temp = sum(cost_temp)\n",
    "            else:\n",
    "                sum_temp = 0\n",
    "        total_cost = total_cost.append(pd.DataFrame([[country, sum_temp]], columns=['Code', 'Total']),\n",
    "                                               ignore_index=True)\n",
    "    return total_cost\n",
    "\n",
    "def sum_col(country_df, index_start, index_end, label):\n",
    "    \"\"\"Helper function for estimate_total_cost, which calculate total cost for a single country\n",
    "    :param country_df: data frame\n",
    "    :param index_start: integer\n",
    "    :param index_end: integer\n",
    "    :param label: string\n",
    "    :returns col_sum: integer\"\"\"\n",
    "    test_col = country_df[index_start:index_end][label]\n",
    "    small_ratio = test_col.isnull().sum() / len(test_col.index)\n",
    "    big_ratio = country_df[label].isnull().sum() / len(country_df.index)\n",
    "    if small_ratio > 0:\n",
    "        if big_ratio < 0.5:\n",
    "            country_df.loc[:,label].interpolate(limit_direction='both', inplace=True)\n",
    "            col_sum = sum(test_col)\n",
    "        else:\n",
    "            col_sum = 0\n",
    "    else:\n",
    "        col_sum = sum(test_col)\n",
    "    return col_sum\n",
    "\n",
    "#estimate total expenses per student in a given country\n",
    "student_total_expenses = estimate_total_cost(edu_data_per_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of our assumption we had to get rid of 10 countries from the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete countries with zero data ([CAN, GRC, HKG, MAC, OAVG, SGP, TUR] [ISR, PER, SVN])\n",
    "student_total_expenses = student_total_expenses[student_total_expenses.Total != 0]\n",
    "\n",
    "#reset index\n",
    "student_total_expenses.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#add column with country name\n",
    "add_country_name(student_total_expenses, code_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show values in descending order\n",
    "student_total_expenses.sort_values(['Total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis for average PISA results and government expenses on education per student \n",
    "\n",
    "Our last step is again regression analysis to check if there is ant relevant correlation between two variables. Let's merge data frames first and check the scatterplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with average PISA results\n",
    "pisa_ave_expenses = merge_df_onCode(all_pisa_2015_ave, student_total_expenses)\n",
    "\n",
    "#plot\n",
    "plot_ave_expenses = show_scatterplot(pisa_ave_expenses, ['Total', 'ave_result'], 'y',\n",
    "                                     'PISA 2015 average result vs. total expenses on education per student',\n",
    "                                     'expenses per student (PPP $)', 'average test result (points)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we're going to need our previous tricks to make the data more 'linear regression friendly'. Let's first take log of expenses values and make another plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take log from expenses\n",
    "pisa_ave_expenses_log = take_log(pisa_ave_expenses, ['Total'])\n",
    "\n",
    "#plot with expenses log\n",
    "plot_ave_expenses_log = show_scatterplot(pisa_ave_expenses_log, ['Total_log', 'ave_result'], 'm',\n",
    "                                        'PISA 2015 average result vs. total expenses on education per student (log)',\n",
    "                                         'expenses per student (log)', 'average test result (points)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it seems that there might be even another outlier apart from Luxembourg - it's Brazil. It has relatively low PISA results for a country with such expenses on education. It may be a sign of some ineffectiveness in education system, but let's not go to far with the interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave LUX out as an outlier\n",
    "pisa_ave_expenses_log_lux = pisa_ave_expenses_log[pisa_ave_expenses_log['Code'] != 'LUX']\n",
    "\n",
    "#leave BRA out as an outlier\n",
    "pisa_ave_expenses_log_lux_bra = pisa_ave_expenses_log_lux[pisa_ave_expenses_log_lux['Code'] != 'BRA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform OLS\n",
    "model_ave_expenses_log_lux_bra = smf.ols(formula='ave_result ~ Total_log', data=pisa_ave_expenses_log_lux_bra).fit()\n",
    "\n",
    "#show summary\n",
    "model_ave_expenses_log_lux_bra.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the adjustments in the data again resulted in increased correlation value - from 0,31 to 0,68 points. Interpreting our model coefficients we can say that after increasing government expenses per student by 1%, the average PISA result should go up by 0.68 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_ave_expenses_log_lux_bra = fit_data_mat(pisa_ave_expenses_log_lux_bra['Total_log'], pisa_ave_expenses_log_lux_bra['ave_result'], 1,\n",
    "                                       'Regression analysis curve fit (without Luxembourg, Brazil)',\n",
    "                                        'expenses per student (log)', 'average test result (points)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after excluding some countries our data fit nicely in the calculated line. There is also another specific thing about this plot, it seems like we have a group of countries centralised around the point with 500 PISA points and 11,5 expenses, while the others, with less spending form a line. It raises a question, is it possible that after reaching some point of expenses per student we do not see to much difference in the assessment score? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Wrapping all the conclusions, we can say that there is a moderate positive relationship between PISA performance and country GDP per capita. This is not only a mattaer of potential, because analyzing real expenses on education gave us the same conclusion. Even though richer countries and those that allocate more resources in education score better, it's still impossible to state the direction of the relation. It may be that the wealth of the country is the reason for high score or it's the situation, where an effective education system produce smart people, who pursue to increasing their country income. The sample analyzed in the text was also not big enough to draw strong conclusions about the relation. The education effectivness itself could be maseured differently. \n",
    "\n",
    "The analysis leaves us with more questions than answers, that is one of the reasons I decided to make a second part of it. There are probably numbers of factors worth analysing in context of PISA performance. We could try some unsupervised learning using clustering methods to identify other relations, which may not seem so obvious. We could check correlation strength of many of them and choose only the most relevant. Then we could try fitting polynomial of different degrees and cross-validate them to check the prediction power of tested models. I hope you enjoyed my work and I'm looking forward to describing the results soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sources:\n",
    "- <p id=\"source_1\">[1] http://www.oecd.org/pisa/pisa-2015-results-in-focus.pdf</p>\n",
    "- [2] https://data.oecd.org/pisa/reading-performance-pisa.htm#indicator-chart\n",
    "OECD (2017), Reading performance (PISA) (indicator). doi: 10.1787/79913c69-en (Accessed on 11 September 2017)\n",
    "- [3] https://data.oecd.org/pisa/mathematics-performance-pisa.htm#indicator-chart\n",
    "OECD (2017), Mathematics performance (PISA) (indicator). doi: 10.1787/04711c74-en (Accessed on 11 September 2017)\n",
    "- [4] https://data.oecd.org/pisa/science-performance-pisa.htm#indicator-chart\n",
    "OECD (2017), Science performance (PISA) (indicator). doi: 10.1787/91952204-en (Accessed on 11 September 2017)\n",
    "- [5] https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-api-documentation\n",
    "- <p id=\"source_6\">[6] http://dss.princeton.edu/online_help/analysis/interpreting_regression.htm</p>\n",
    "- <p id=\"source_7\">[7] http://www.businessinsider.com/the-23-richest-countries-in-the-world-2015-7?IR=T</p>\n",
    "- [8] http://data.uis.unesco.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
